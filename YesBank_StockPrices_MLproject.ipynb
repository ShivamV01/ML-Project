{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "VFOzZv6IFROw",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivamV01/ML-Project/blob/main/YesBank_StockPrices_MLproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Shivam Vishwakarma\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accurately forecasting the closing price of Yes Bank's stock is a critical challenge for investors, market participants, and stakeholders due to the bank's recent tumultuous history. As a prominent private sector bank in India, Yes Bank has faced significant financial distress, marked by a substantial accumulation of bad loans and allegations of fraudulent activities. This has resulted in regulatory intervention by the Reserve Bank of India, creating an environment of uncertainty and complexity surrounding the bank's stock price trajectory.\n",
        "\n",
        "This project endeavors to address this challenge by harnessing a comprehensive dataset encompassing monthly stock price information from the bank's inception. The dataset includes essential metrics such as the closing, starting, highest, and lowest prices for each month, providing a rich source of historical data for analysis. The overarching aim is to develop robust predictive models capable of capturing the nuanced dynamics and trends in Yes Bank's stock prices, even in the face of recent adverse events and the associated volatility.\n",
        "\n",
        "To achieve this, the project will employ a multifaceted approach, leveraging a variety of modeling techniques that include time series models and regression methods. A thorough evaluation will be conducted to assess the efficacy of these models in accurately predicting Yes Bank's closing stock price. Furthermore, the models will be rigorously tested to gauge their ability to incorporate the impact of pivotal events, such as fraud cases involving the bank's founders or regulatory actions taken by the Reserve Bank of India.\n",
        "\n",
        "The successful prediction of Yes Bank's closing stock price holds the potential to offer invaluable insights to stakeholders, empowering them to make well-informed investment decisions. By navigating the complex and unpredictable nature of Yes Bank's stock performance, this project aims to deepen the understanding of its financial standing and contribute to effective decision-making in the financial markets."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main objective of this project is to develop a robust and accurate predictive model that can effectively forecast the closing price of Yes Bank's stock. The challenge lies in understanding and capturing the complex dynamics and trends in the stock prices, considering various factors such as the historical trend of an increasing price followed by a sudden decline after 2018.\n",
        "\n",
        "One of the key challenges in developing the predictive model is addressing the issue of multicollinearity present in the dataset. Multicollinearity occurs when there is a high correlation between independent variables, which can lead to difficulties in interpreting the model and can affect the accuracy of the predictions. Therefore, the model should incorporate techniques to handle multicollinearity and ensure that the independent variables are appropriately considered in the prediction process.\n",
        "\n",
        "Furthermore, the model should account for significant events that have had an impact on Yes Bank's stock performance. This includes events such as fraud cases involving the bank's founders and regulatory interventions by the Reserve Bank of India. These events can significantly influence the stock prices, and it is crucial for the predictive model to capture and reflect their effects accurately.\n",
        "\n",
        "In terms of performance, the model should aim for a high level of accuracy in forecasting the closing price of Yes Bank's stock. The 99% accuracy achieved by the K-Nearest Neighbors (KNN) Regression model serves as a benchmark, indicating the target accuracy that the developed model should strive to achieve. By achieving high accuracy, the predictive model can provide valuable insights to stakeholders, investors, and market participants, enabling them to make informed decisions and effectively manage their investments in Yes Bank's stock.\n",
        "\n",
        "Overall, this project seeks to develop a predictive model that addresses the complexities and challenges associated with forecasting Yes Bank's stock prices. The ultimate goal is to provide stakeholders with a reliable tool that can enhance their understanding of the stock's future performance and support them in making well-informed investment decisions."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Import Plotly graph objects for interactive visualizations\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# pending to import sklearn and module"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "link = \"/content/drive/MyDrive/Almabetter/Capstone video and projects by me/Module 6/data_YesBank_StockPrices.csv\"\n",
        "\n",
        "stock_df = pd.read_csv(link)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QqYXRDMJddNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, columns = stock_df.shape\n",
        "print(f\"Numbers of Rows: {rows}\")\n",
        "print(f\"Numbers of Columns: {columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "stock_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicated_count=stock_df.duplicated().sum()\n",
        "print(duplicated_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "Null_values = stock_df.isnull().sum()\n",
        "print(Null_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "columns_to_fill = ['Open', 'High','Low', 'Close']  #Although there is no null data\n",
        "df_filled = stock_df[columns_to_fill].fillna(stock_df[columns_to_fill].mean())\n",
        "print(df_filled)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The data includes High, Low, Open, Close price of a specific stock on a daily basis. It also includes the date and their High, Low, Open, Close price.\n",
        "*   The data has 185 Rows and 5 Columns. There also no null values in the dataframe. Also there is no duplicate values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "stock_df.columns.tolist()"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "stock_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   There are total 185 elements in each columns.\n",
        "*   The min and max value for following columns are:\n",
        "    1.   Open   = 10 to 369.95\n",
        "    2.   High   = 11.24 to 404\n",
        "    3.   Low    = 5.55 to 345.5\n",
        "    4.   Close  = 9.98 to 367.9\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in stock_df.columns:\n",
        "    unique_values = stock_df[column].unique()\n",
        "    print(f\"Unique values in column '{column}': {unique_values}\")\n",
        "stock_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for variable in stock_df.columns:\n",
        "  print(f\"The unique values for the '{variable}' variable are:\\n\\n {stock_df[variable].unique()}\\n\\n\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving a copy of the original dataframe\n",
        "og_df = stock_df.copy()"
      ],
      "metadata": {
        "id": "aucvY76VOxyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Type Correction**"
      ],
      "metadata": {
        "id": "q2v2LcVlFN9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset does not contain any duplicate or null values, we do not need to perform any operations to treat them. We can proceed to outlier detection and dealing with them.\n",
        "\n",
        "However, the datatype of the Date column is currently object. We need to change it to datetime format. This is because the Date column represents a date and time, and the object datatype is not sufficient to represent this type of data.\n",
        "\n",
        "To change the datatype of the Date column, we can use the pd.to_datetime() function. For example, the following code would change the datatype of the Date column to datetime:"
      ],
      "metadata": {
        "id": "8eEjq87QFVsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the exact datatype of the entries under the 'Date' column\n",
        "type(stock_df['Date'][0])"
      ],
      "metadata": {
        "id": "wQuhYvkvFUG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing date colunn datatype to datetime format.\n",
        "from datetime import datetime\n",
        "\n",
        "# parsing date which is string of format %b-%y to datetime (%b for Month as locale’s abbreviated name and %y for Year without century as a zero-padded decimal number.\n",
        "stock_df['Date'] = stock_df['Date'].apply(lambda x: datetime.strptime(x, '%b-%y'))"
      ],
      "metadata": {
        "id": "eiKO1ezTFaeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the datatype of the columns after changing datatype of date\n",
        "# using 'info()' method\n",
        "print(stock_df.info())"
      ],
      "metadata": {
        "id": "yhZJtsbFFdBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the datatype of the columns after changing datatype of date\n",
        "stock_df.dtypes"
      ],
      "metadata": {
        "id": "Gqiy3XqDFfpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the 'Date' column as the index\n",
        "stock_df = stock_df.set_index('Date')"
      ],
      "metadata": {
        "id": "mvep1JgVFht-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the background color of the DataFrame to a gradient\n",
        "# using 'style.background_gradient()' method\n",
        "stock_df.head().style.background_gradient(cmap='hot')"
      ],
      "metadata": {
        "id": "jT4i0wfdFjz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the background color of the DataFrame to a gradient\n",
        "# using `style.background_gradient()` method\n",
        "stock_df.tail().style.background_gradient(cmap='hot')"
      ],
      "metadata": {
        "id": "8BrjtXQeFseR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_variable = ['Close']\n",
        "independent_variables = list(stock_df.columns[:-1])"
      ],
      "metadata": {
        "id": "mhRMgtejFvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon examining the provided dataframe, it becomes apparent that all the columns exclusively consist of numerical data. There is an absence of any categorical data in the dataset, which means that the information available for analysis primarily comprises quantitative values. This characteristic enables direct application of numerical calculations, statistical analyses, and modeling techniques to the dataset. The lack of categorical data simplifies data processing and ensures a streamlined approach when performing quantitative analyses.\n",
        "\n",
        "Furthermore, during the examination of the dataset, it is evident that outliers are present. These outliers are data points that significantly deviate from the majority of the data. Before proceeding with modeling or conducting further analysis, it is crucial to address these outliers. Dealing with outliers involves assessing their impact on the data and making decisions regarding appropriate actions, such as removing or transforming them. By addressing the outliers, we can enhance the robustness and reliability of our models and analyses."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Barchart month of Year VS Open price"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Set the figure size before plotting\n",
        "plt.figure(figsize=(40, 10))  # Increase width and height of the chart\n",
        "\n",
        "# Create the barplot\n",
        "sns.barplot(x='Date', y='Open', data=stock_df)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Month VS Open price', fontsize=16)  # Increased font size for clarity\n",
        "plt.xlabel('Date', fontsize=12,fontweight='bold')\n",
        "plt.ylabel('Open price', fontsize=12)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=90, fontweight='bold')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to check the opening price for every year I have chosen this chart.\n",
        "It will represent the data with respect to the first year of available data.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   In the Year of 2018\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Months VS Close price"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Set the figure size before plotting\n",
        "plt.figure(figsize=(40, 10))  # Increase width and height of the chart\n",
        "\n",
        "# Create the barplot\n",
        "sns.barplot(x='Date', y='Close', data=stock_df)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Month VS Close price', fontsize=16)  # Increased font size for clarity\n",
        "plt.xlabel('Date', fontsize=12,fontweight='bold')\n",
        "plt.ylabel('Close price', fontsize=12)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=90, fontweight='bold')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 candle stick graph with price movement"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Create a Figure object with Candlestick chart\n",
        "fig = go.Figure(go.Candlestick(\n",
        "    x = stock_df.index,            # x-axis values (dates)\n",
        "    open = stock_df['Open'],       # open prices\n",
        "    high = stock_df['High'],       # high prices\n",
        "    low = stock_df['Low'],         # low prices\n",
        "    close = stock_df['Close']      # close prices\n",
        "))\n",
        "\n",
        "# Update the layout of the figure with a title\n",
        "fig.update_layout(\n",
        "    title={'text': 'Describing the Price Movements', 'x': 0.5, 'y': 0.95, 'font': {'color': 'white'}},\n",
        "    xaxis=dict(title='Year', title_font={'color': 'white'}, tickfont={'color': 'white'}),\n",
        "    yaxis=dict(title='Price', title_font={'color': 'white'}, tickfont={'color': 'white'}),\n",
        "    width=1450,\n",
        "    height=1000,\n",
        "    plot_bgcolor='rgb(36, 40, 47)',  # Set the background color to a professional dark gray\n",
        "    paper_bgcolor='rgb(51, 56, 66)'  # Set the paper color\n",
        ")\n",
        "\n",
        "\n",
        "# Show the figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Candlestick chart was chosen as our preferred visualization for analyzing price movements due to its effectiveness in conveying essential information. It provides a visual representation of open, high, low, and close prices, making it a popular choice for us in financial analysis, particularly in the context of stocks and other assets. The Candlestick chart excels in capturing market sentiment and price trends, as each candlestick represents a specific time interval. By observing the color and shape of the candlesticks, we can quickly discern whether prices increased or decreased during that interval. The high and low points of the candlesticks indicate the highest and lowest prices reached within the given period, while the body represents the opening and closing prices. These features enable us to identify patterns, trends, and potential price reversals, facilitating informed decisions regarding asset buying or selling. The larger graph size further enhances visibility, allowing for a more detailed analysis of the price movements depicted by the Candlestick chart. Overall, the Candlestick chart is a valuable tool for us to understand and interpret price dynamics in financial markets"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of Yes Bank stock prices reveals a distinct pattern. Prior to 2018, the stock exhibited a consistent upward trend, indicating positive growth and reflecting investor optimism. However, a significant decline occurred after this period, primarily attributed to the Yes Bank fraud case involving Rana Kapoor, the former CEO.\n",
        "\n",
        "Leading up to 2018, the stock experienced a continuous rise, demonstrating favorable market conditions and investor confidence. However, the revelation of the fraud case involving Rana Kapoor had a profound impact on the stock's performance. This event marked a turning point, as the stock prices sharply declined.\n",
        "\n",
        "The fraud case involving Rana Kapoor significantly affected investor sentiment, eroding trust and confidence in Yes Bank. Consequently, the stock's value experienced a notable decrease, reflecting the negative repercussions of the scandal on the company's reputation and financial stability.\n",
        "\n",
        "Overall, the analysis highlights the contrasting trends in Yes Bank's stock prices. Pre-2018, there was a consistent upward trajectory, while the post-2018 period witnessed a significant decline due to the repercussions of the fraud case involving Rana Kapoor."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of the Yes Bank fraud case on the stock prices is evident in the abrupt change in the trend. The case brought about increased scrutiny and regulatory interventions, causing a negative sentiment surrounding the bank's future prospects. Consequently, investors reacted by selling off their shares, leading to a rapid decline in the stock prices.\n",
        "\n",
        "It is important to consider external factors, such as legal proceedings and market sentiment, when interpreting the observed drop in stock prices. The Yes Bank fraud case involving Rana Kapoor significantly affected investor perception and had a direct impact on the stock's value, resulting in the observed downturn in stock prices after 2018."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Line plot showcasing variations in each feature over the years"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Plotting line graph wrt Date and Low prices\n",
        "fig = px.line(og_df, x=\"Date\", y=\"Low\")\n",
        "\n",
        "# Add additional traces for Open, Close, and High prices\n",
        "fig.add_scatter(x=og_df['Date'], y=og_df['Open'], name=\"Open\",\n",
        "                line_color='lime', marker_color='hotpink', marker_size=10)\n",
        "fig.add_scatter(x=og_df['Date'], y=og_df['Close'], name=\"Close\",\n",
        "                line_color='cyan', marker_color='magenta', marker_size=10)\n",
        "fig.add_scatter(x=og_df['Date'], y=og_df['High'], name=\"High\",\n",
        "                line_color='gold', marker_color='deepskyblue', marker_size=10)\n",
        "fig.add_scatter(x=og_df['Date'], y=og_df['Low'], name=\"Low\",\n",
        "                line_color='orange', marker_color='chartreuse', marker_size=10)\n",
        "\n",
        "# Update the layout of the plot\n",
        "fig.update_layout(\n",
        "    title={'text': \"Yes Bank Prices with Respect to Year\", 'x': 0.5, 'y': 0.95, 'xanchor': 'center', 'yanchor': 'top', 'font': {'color': 'white'}},\n",
        "    xaxis_title=\"Year\",\n",
        "    yaxis_title=\"Price\",\n",
        "    width=1400,\n",
        "    height=800,\n",
        "    plot_bgcolor='rgb(36, 40, 47)',  # Set the dark blue background color of the plot\n",
        "    paper_bgcolor='rgb(51, 56, 66)',  # Set the dark blue background color of the paper area\n",
        "    font_color='white',  # Set the font color to white\n",
        "    legend=dict(x=0.02, y=0.98, bgcolor='rgba(255, 255, 255, 0.7)', bordercolor='gray', borderwidth=1, font={'color': 'white'}),\n",
        "    margin=dict(l=50, r=50, t=100, b=50),\n",
        "    xaxis=dict(tickangle=90)  # Rotate x-axis labels by 90 degrees\n",
        ")\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart chosen for this analysis is a combination of line and scatter plots. This chart type is suitable for visualizing the individual changes in Open, High, Low, and Close prices of the Yes Bank stock over time. By utilizing line plots, we can observe the overall trends and patterns, while scatter plots allow us to identify specific data points. The chart effectively presents the data by distinguishing each price variable with unique line colors and marker styles. The layout of the plot is optimized to include a centered title, clear axis labels, and an appropriate size. Additionally, the choice of color scheme and background enhances visual appeal and readability. Overall, this chart enables a comprehensive analysis of the Yes Bank stock prices, aiding in the identification of trends, patterns, and potential insights for informed decision-making."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the expected dip in the price variables after 2018 is prominently visible in the chart. The line graph shows a notable decrease in the Open, High, Low, and Close prices of the Yes Bank stock following the specified timeframe. This decline can be attributed to various factors, such as the impact of the Yes Bank fraud case involving Rana Kapoor, which adversely affected investor sentiment and confidence in the bank. The scatter plots further accentuate the dip, as they highlight individual data points that deviate significantly from the preceding upward trend. By visually representing the price variables over time, the chart effectively showcases the substantial decrease in prices after 2018, emphasizing the challenging period faced by Yes Bank and the subsequent decline in its stock value."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact by providing valuable information for decision-making and strategic planning. By analyzing the Yes Bank stock price data and observing the significant dip after 2018, businesses and investors can adjust their strategies accordingly. These insights can guide them in making informed decisions about investing in Yes Bank or adjusting their existing holdings. Additionally, the insights can alert businesses to the need for diligent risk management practices and thorough due diligence when evaluating financial institutions.\n",
        "\n",
        "Regarding insights leading to negative growth, the significant dip in the Yes Bank stock prices after 2018 can be seen as a negative growth trend. The decline in stock prices can be attributed to various factors, including the Yes Bank fraud case involving Rana Kapoor. This event had a detrimental impact on investor sentiment and eroded trust in the bank, resulting in a decrease in its stock value. The negative growth observed in this scenario highlights the importance of maintaining ethical practices, strong corporate governance, and transparency within financial institutions. It also underscores the potential consequences of fraud and misconduct on the overall growth and stability of a business."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Distribution of dependent variable Close Price of stock."
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Set the figure size and title\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.suptitle('Overall Distribution of Each Variable', color='white')\n",
        "\n",
        "# Define the color list for each variable (using Yes Bank color scheme)\n",
        "color_list = ['#003366', '#FF6600', '#99CC00', '#FFCC00']\n",
        "\n",
        "# Set the dark theme background color\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "for i, column in enumerate(stock_df.columns):\n",
        "    # Create subplots\n",
        "    ax1 = plt.subplot(2, 2, i + 1)\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot histogram\n",
        "    sns.histplot(stock_df[column], color=color_list[i], ax=ax1)\n",
        "\n",
        "    # Plot KDE curve\n",
        "    sns.kdeplot(stock_df[column], color=color_list[i], ax=ax2)\n",
        "\n",
        "    # Set gridlines\n",
        "    ax1.grid(which='major', alpha=0.5)\n",
        "    ax1.grid(which='minor', alpha=0.5)\n",
        "\n",
        "    # Add vertical lines for mean and median\n",
        "    plt.axvline(stock_df[column].mean(), color='white', linestyle='dashed', linewidth=1.5)\n",
        "    plt.axvline(stock_df[column].median(), color='yellow', linestyle='dashed', linewidth=1.5)\n",
        "\n",
        "# Set the background color of the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)\n",
        "\n",
        "# Set the background color of the axes\n",
        "paper_bgcolor = (51/255, 56/255, 66/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "for ax in plt.gcf().get_axes():\n",
        "    ax.set_facecolor(paper_bgcolor)\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chosen chart, a combination of histograms and KDE plots, effectively visualizes the distribution of each variable in the dataset. It allows for the examination of central tendency, spread, and shape of the distributions. The subplots enable easy comparison between variables. The color scheme aligns with the Yes Bank branding. The chart aids in data exploration and analysis, providing insights into skewness, multimodality, and outliers. It is a concise and efficient representation of the overall distribution of the variables. The histograms show frequency distribution, while the KDE plots provide a smooth curve. The chart is visually cohesive and facilitates pattern identification. It serves as a valuable tool for understanding the dataset and identifying relationships between variables."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distributions of open, high, low, and close in the chart are positively skewed. This indicates that the majority of data points are concentrated on the left side of the distributions, with a tail extending towards larger values on the right side. The histograms and KDE plots clearly show this skewness. Positive skewness suggests that the variables have a tendency for higher values, but with fewer occurrences. The presence of positive skewness may indicate bounded or restricted variables, resulting in an accumulation of values on the lower end and a tail of relatively larger values. Proper consideration of the positive skewness is important for accurate data analysis and modeling, potentially requiring transformations or alternative techniques to account for the skewness."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about the positively skewed distributions of open, high, low, and close prices can have a positive business impact by informing strategic decision-making and identifying potential buying opportunities. However, it is important to note that positive skewness does not directly imply negative growth. Negative growth would require a comprehensive analysis considering various factors beyond skewness, such as trends, market conditions, and external influences. Therefore, it is not justified to conclude specific insights leading to negative growth based solely on the skewness of the distributions. Further analysis is needed to assess any potential negative impacts on business growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Boxplots: Studying the Outliers"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "boxplot = stock_df.boxplot(column=['Open', 'High', 'Low', 'Close'], grid=False, notch=True)\n",
        "\n",
        "# Change the line color to white\n",
        "for item in boxplot.findobj(plt.Line2D):\n",
        "    item.set_color('white')  # Set the color of the lines to white\n",
        "\n",
        "# Add title to the plot\n",
        "plt.title(\"Outliers in Various Features\")\n",
        "\n",
        "# Change the background color of the plot\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "paper_bgcolor = (51/255, 56/255, 66/255, 1)\n",
        "\n",
        "fig.patch.set_facecolor(paper_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a boxplot, which was chosen for its effectiveness in comparing multiple variables, detecting outliers, visualizing distributions, and providing a concise summary of the data. The notch feature adds a confidence interval around the median, enhancing comparison. The boxplot's space efficiency allows for displaying multiple variables in a compact manner. Removing the gridlines reduces visual clutter. The code also demonstrates customization flexibility, such as changing line color to white. Overall, the boxplot is a suitable choice for analyzing and comparing the Open, High, Low, and Close prices of the stock."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presence of outliers in each of the features indicates the existence of extreme values that deviate significantly from the overall pattern of the data. These outliers can potentially impact the model fitting process and the accuracy of the predictions. Therefore, it is crucial to address these outliers before proceeding with model fitting.\n",
        "\n",
        "To handle outliers, various approaches can be employed, such as removing them from the dataset, transforming the data using robust statistical techniques, or imputing them with more representative values. The choice of the method depends on the nature of the data and the specific requirements of the analysis.\n",
        "\n",
        "Handling outliers helps to ensure that the model captures the underlying patterns and relationships accurately, leading to more reliable predictions and interpretations. It also improves the robustness of the model against extreme observations that may introduce bias or noise. Properly addressing outliers contributes to the overall validity and integrity of the analysis, enhancing the reliability of the model fitting process and subsequent predictions."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bivarient analysis**"
      ],
      "metadata": {
        "id": "CJvk9XPXHe8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Scatter Plot to see the Best Fit line"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Function to create scatter plots with correlation lines\n",
        "def create_scatter_plot(col, df):\n",
        "    fig = plt.figure(figsize=(20, 10))  # Create a new figure with the specified size\n",
        "    ax = fig.gca()  # Get the current axes\n",
        "    feature = df[col]  # Extract the data for the given column\n",
        "    label = df['Close']  # Extract the 'Close' column data\n",
        "    correlation = feature.corr(label)  # Calculate the correlation between the two columns\n",
        "    plt.scatter(x=feature, y=label, marker=\"*\", c=\"b\", s=label/20)  # Create a scatter plot with blue markers\n",
        "    plt.xlabel(col)  # Set the label for the x-axis\n",
        "    plt.ylabel('Close')  # Set the label for the y-axis\n",
        "    ax.set_title(col + ' Vs. Close' + '         Correlation: ' + str(round(correlation, 2)), fontsize=16)  # Set the title of the plot\n",
        "    z = np.polyfit(df[col], df['Close'], 1)  # Fit a linear regression line to the data\n",
        "    y_hat = np.poly1d(z)(df[col])  # Generate the y-values for the regression line\n",
        "\n",
        "    plt.plot(df[col], y_hat, \"r\", lw=1)  # Plot the regression line in red\n",
        "\n",
        "    # Add a comment\n",
        "    plt.annotate('The correlation coefficient is {}.'.format(round(correlation, 2)), (200, 0.2), fontsize=10)\n",
        "\n",
        "    # Change the shape of the marker\n",
        "    plt.scatter(df[col], df['Close'], marker=\"*\", c=\"b\", s=label/20)  # Create another scatter plot with blue markers\n",
        "\n",
        "    # Change the size of the marker\n",
        "    plt.grid(alpha=0.3)  # Add grid lines with transparency (alpha=0.3)\n",
        "    plt.xticks(np.arange(min(df[col]), max(df[col]), 100))  # Set the x-axis ticks\n",
        "    plt.yticks(np.arange(min(df['Close']), max(df['Close']), 10))  # Set the y-axis ticks\n",
        "\n",
        "    # Set the background colors\n",
        "    plot_bgcolor = (36/255, 40/255, 47/255)  # RGB values divided by 255\n",
        "    paper_bgcolor = (51/255, 56/255, 66/255)  # RGB values divided by 255\n",
        "\n",
        "    fig.patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "    plt.show()  # Display the plot\n",
        "\n",
        "# Loop through the columns and create scatter plots\n",
        "for col in ['Open', 'High', 'Low']:\n",
        "    create_scatter_plot(col, stock_df)  # Call the function for each column in the loop"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using scatter plots with a best fit line allows for visualizing the relationship between numerical features and the 'Close' price. The correlation coefficient quantifies the strength of the relationship. The best fit line provides an estimate of the trend and predictive power. The plot aids interpretation and communication of the relationship to stakeholders. Annotations, such as the correlation coefficient, provide valuable insights. Customization enhances clarity and aesthetics. The plots help identify potential predictors and support analysis and decision-making in stock market analysis."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the scatter plots with the best fit line, it is evident that all the independent variables show a linear relationship with the dependent variable, 'Close'. This indicates that there is a consistent and predictable relationship between these variables.\n",
        "\n",
        "The presence of a linear relationship has important implications in data analysis and modeling. It suggests that changes in the independent variables can be associated with proportional changes in the dependent variable. This knowledge can be leveraged to build regression models, make predictions, and understand the impact of the independent variables on the 'Close' price."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The business impact of identifying linear relationships between the independent variables and the dependent variable is significant. It allows for better decision-making and forecasting in the context of stock market analysis. Here are a few potential implications:\n",
        "\n",
        "Prediction and Forecasting: With a clear understanding of the linear relationships, regression models can be developed to predict future 'Close' prices based on the values of the independent variables. This can assist in forecasting stock performance and informing investment decisions.\n",
        "\n",
        "Risk Assessment: By analyzing the strength and direction of the linear relationships, it becomes possible to assess the risk associated with changes in the independent variables. This knowledge can aid in risk management and portfolio optimization strategies.\n",
        "\n",
        "Feature Selection: Identifying the linear relationships helps in determining the most influential independent variables that impact the 'Close' price. This knowledge can guide feature selection and variable prioritization in future analyses or model development.\n",
        "\n",
        "Strategy Development: The linear relationships can provide insights into the factors driving stock price movements. This information can be utilized to develop trading strategies, identify patterns, and make informed investment decisions.\n",
        "\n",
        "By recognizing and understanding the linear relationships between the independent variables and the dependent variable, businesses and investors can gain valuable insights into the dynamics of stock prices. This can lead to improved forecasting accuracy, risk management, and decision-making in the context of financial markets."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Pair Plot"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Create a pair plot to explore relationships in the stock_df DataFrame\n",
        "sns.pairplot(stock_df)\n",
        "\n",
        "# Set the background colors for the figure and axes\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "paper_bgcolor = (51/255, 56/255, 66/255, 1)\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "plt.gca().patch.set_facecolor(paper_bgcolor)  # Set the background color of the axes\n",
        "\n",
        "# Display the pair plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot graph is used for analyzing the Yes Bank stock price because it helps explore relationships, detect patterns and trends, and identify outliers. It allows for correlation analysis and understanding of data distributions. The visualization of time series data is facilitated, aiding in the identification of long-term trends. Pair plots are visually appealing and effective for communicating findings to stakeholders. They can generate hypotheses and compare variables with other relevant factors. Pair plots support exploratory data analysis, serving as a starting point for further analysis. The choice to use this visualization technique depends on research questions, dataset characteristics, and analyst judgment. Combining visual exploration with statistical analysis is recommended for a comprehensive understanding."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variables Open, High, and Low show a strong correlation with the Close variable, indicating a close relationship between the stock's opening, highest, lowest, and closing prices. The Open, High, and Low variables also exhibit a high correlation with each other, suggesting they move in sync and share similar trends. These correlations provide valuable insights for analyzing the Yes Bank stock and can serve as predictors of the closing price. The relationships highlight the interdependencies within the stock market and the potential impact of external factors. Understanding these connections aids in making informed decisions and identifying patterns for forecasting future stock price movements. However, it's important to consider that correlation does not imply causation, and comprehensive analysis requires additional factors and considerations."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the strong correlations between the Open, High, Low, and Close variables can have a positive business impact. They aid in making informed decisions and predicting the closing price, improving investment strategies for Yes Bank stock. However, correlation alone does not guarantee success, and comprehensive analysis is necessary, considering other factors and potential limitations. The insights do not directly lead to negative growth; rather, misinterpretation or over-reliance on correlations without considering other factors can result in negative outcomes. Thorough analysis, risk management, and understanding market conditions are crucial for mitigating risks and ensuring positive growth.\n",
        "\n",
        "To address multicollinearity, we drop the variable with the highest VIF. We prioritize retaining variables that have the strongest correlation with the dependent variable while having the least correlation with other independent variables. This improves model performance and interpretation by reducing interdependence between variables. The decision aligns with the objective of selecting the most influential predictors and enhancing prediction accuracy."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values were found in the dataset, as confirmed earlier. Therefore, there is no requirement for missing values imputation techniques. The dataset is complete, allowing for direct analysis without the need to handle missing data."
      ],
      "metadata": {
        "id": "TtwMOb9WY4Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Create a figure with a size of 10x6 inches\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Add a super title to the plot\n",
        "plt.suptitle('Studying the Outliers after Log Transformation', color='white', fontsize=16)\n",
        "\n",
        "# Define a list of colors for the boxplots\n",
        "color_list = ['blue', 'green', 'red', 'grey','orange'] # Added a new color to the list\n",
        "\n",
        "# Calculate the number of rows needed for the subplots\n",
        "num_rows = int(np.ceil(len(stock_df.columns) / 2)) # Calculate the number of rows based on the number of columns\n",
        "\n",
        "# Iterate over each column in the dataframe\n",
        "for i, column in enumerate(stock_df.columns[1:], start=1):\n",
        "    # Create subplots for each column\n",
        "    plt.subplot(num_rows, 2, i) # Use num_rows to create the correct number of rows\n",
        "\n",
        "    # Check if the column contains numeric data\n",
        "    if pd.api.types.is_numeric_dtype(stock_df[column]):\n",
        "        # Apply a log transformation to the column and create a boxplot\n",
        "        sns.boxplot(x=np.log10(stock_df[column]), color=color_list[i])\n",
        "\n",
        "        # Add a title to each subplot\n",
        "        plt.title(column, color='white')\n",
        "\n",
        "\n",
        "# Adjust the layout of the subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying the log transformation to the features, there are no outliers remaining. The boxplots show no extreme values beyond the whiskers. The log transformation successfully reduced the impact of outliers and normalized the data. However, it is important to consider other factors and limitations in the analysis."
      ],
      "metadata": {
        "id": "Kp4Fw17AcL-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation was applied as a treatment for outliers. This approach not only addresses outliers but also helps to alleviate skewness in the features' distribution. By using log transformation, two problems - outlier treatment and skewness correction - are tackled simultaneously, providing a consolidated solution. This technique aids in normalizing the data and improving the suitability of the features for analysis and modeling purposes."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Create an empty dataframe to store the VIF for each feature\n",
        "vif_df = pd.DataFrame()\n",
        "\n",
        "# Assign the feature names to the 'Features' column\n",
        "vif_df['Features'] = stock_df.iloc[:, :-1].columns.tolist()\n",
        "\n",
        "# Calculate the VIF for each feature and store it in the 'VIF' column\n",
        "vif_df['VIF'] = [variance_inflation_factor(stock_df.iloc[:, :-1].values, i) for i in range(len(stock_df.iloc[:, :-1].columns))]\n",
        "\n",
        "# Display the dataframe containing the features and their corresponding VIF values\n",
        "vif_df"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VIF values for all the features indicate high multicollinearity. However, considering the small size of the dataset and having only three numerical independent variables, there is limited potential for feature manipulation that could be beneficial. With the absence of categorical variables, the scope for feature engineering or transformation is constrained. Therefore, the focus should be on alternative modeling approaches or additional data collection to address the issue of multicollinearity."
      ],
      "metadata": {
        "id": "3yiEMit7jtt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the dataset's small size, any form of feature selection becomes impractical. Given the limited number of observations, attempting to reduce the feature space may lead to unreliable or biased results. Therefore, it is advisable to retain all available features for analysis or modeling purposes."
      ],
      "metadata": {
        "id": "Pl8UsdrUjwzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the skewed distribution of the features, a data transformation is necessary to approximate a normal distribution. In this case, a log transformation will be applied. This transformation aims to reduce skewness and make the data more symmetrical. Furthermore, as observed earlier, the log transformation also aids in handling outliers. By employing this transformation, we can simultaneously improve the normality of the data distribution and mitigate the impact of outliers."
      ],
      "metadata": {
        "id": "zSdDG0OBj35w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each column in the dataframe\n",
        "for column in stock_df.columns:\n",
        "    # Apply a log transformation to the column using np.log10()\n",
        "    stock_df[column] = np.log10(stock_df[column])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a size of 15x10 inches\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Set the background colors for the figure and axes\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "paper_bgcolor = (51/255, 56/255, 66/255, 1)\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Add a super title to the plot\n",
        "plt.suptitle('Overall Distribution of Each Variable after Log Transformation', color='white', fontsize=16)\n",
        "\n",
        "color_list = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "for i, column in enumerate(stock_df.columns):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    ax1 = plt.gca()\n",
        "    sns.histplot(stock_df[column], color=color_list[i], ax=ax1)\n",
        "    ax2 = ax1.twinx()\n",
        "    sns.kdeplot(stock_df[column], color=color_list[i], ax=ax2)  # Overlapping the KDE plot on the histogram.\n",
        "\n",
        "    # Set the background color of the axes\n",
        "    ax1.patch.set_facecolor(paper_bgcolor)\n",
        "    ax2.patch.set_facecolor(paper_bgcolor)\n",
        "\n",
        "    # Add gridlines\n",
        "    plt.grid(which='major', alpha=0.5)\n",
        "    plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "    # Add dashed lines for mean and median\n",
        "    plt.axvline(stock_df[column].mean(), color='purple', linestyle='dashed', linewidth=1.5)\n",
        "    plt.axvline(stock_df[column].median(), color='orange', linestyle='dashed', linewidth=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PiybFWQ8j-sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the log transformation, the distributions of the features appear to be closer to a normal distribution compared to their previous state. The mean (indicated by the purple vertical line) and the median (represented by the yellow vertical line) are nearly equal for each feature. This alignment suggests that the log transformation successfully reduced the skewness and brought the data closer to symmetry. The convergence of the mean and median highlights the relative balance in the distribution, indicating a more representative central tendency. Overall, these observations indicate an improved approximation to a normal distribution after the log transformation."
      ],
      "metadata": {
        "id": "mMj5Eoc4kD-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset is already small in size, there is no need for dimensionality reduction techniques. With a limited number of observations, attempting to reduce the number of features may not provide significant benefits and could potentially lead to loss of valuable information. Therefore, it is advisable to retain all the available features for analysis or modeling purposes without applying dimensionality reduction methods."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assign the independent and dependent variables to X and y, respectively\n",
        "X = stock_df[independent_variables]\n",
        "y = stock_df[dependent_variable]\n",
        "\n",
        "# Split the data into training and testing datasets using a test size of 0.2 (20%)\n",
        "# Set random_state to 0 for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model effectively, an 80:20 split ratio is being employed, allocating 80% of the data for training and 20% for testing. However, considering the small dataset size, it may be beneficial to acquire more data for training purposes. Increasing the training data size helps improve the model's ability to learn and generalize from the patterns present in the data. Gathering additional data can enhance the model's performance, reduce the risk of overfitting, and provide a more comprehensive representation of the underlying relationships within the dataset."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Scaling"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training data (X_train) using fit_transform\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale the testing data (X_test) using transform\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "e0KTwVZKkhJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the training dataset\n",
        "X_train[0: 10]"
      ],
      "metadata": {
        "id": "P-9YYH66kjgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The StandardScaler is utilized in this code snippet because we are primarily working with linear regression, which assumes normally distributed features. By applying the StandardScaler, we can standardize the features, transforming them to have a mean of 0 and a standard deviation of 1. This process aligns with the assumptions of linear regression and helps ensure that the features are on a similar scale, facilitating accurate model fitting and interpretation."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a powerful machine learning algorithm that falls under the category of supervised learning. It is specifically designed for regression tasks, where the goal is to predict a continuous target variable based on independent variables. In regression analysis, the algorithm establishes a relationship between the predictor variables and the target variable to make accurate predictions.\n",
        "\n",
        "The primary objective of Linear Regression is to identify and quantify the relationship between variables. By examining the patterns and trends in the data, the algorithm enables us to understand how changes in one variable affect the target variable. This understanding is crucial for making informed decisions and forecasting future outcomes.\n",
        "\n",
        "Linear Regression is widely employed in various domains, including finance, economics, social sciences, and engineering. It finds applications in areas such as sales forecasting, housing price prediction, demand estimation, and trend analysis. By leveraging the insights gained from analyzing the relationship between variables, Linear Regression empowers us to make reliable forecasts and make informed business decisions.\n",
        "\n",
        "In summary, Linear Regression is a versatile algorithm that allows us to explore the relationships between variables and make predictions based on those relationships. Its ability to model the dependencies between variables makes it a valuable tool for understanding data and making accurate forecasts in numerous fields."
      ],
      "metadata": {
        "id": "zbDjdfqulJrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create an instance of the LinearRegression model\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Fit the Linear Regression model to the training data\n",
        "linear_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_lin = linear_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "hBACwKSPlZqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters\n",
        "print(\"Coefficients:\", linear_reg.coef_)\n",
        "print(\"Intercept:\", linear_reg.intercept_)"
      ],
      "metadata": {
        "id": "J6rlLFdnlcBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Linear Regression model\n",
        "plt.plot(10**y_pred_lin, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Linear Regression\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8q49j88clgX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression aims to establish a linear connection between the independent and dependent variables by minimizing the sum of squared differences between the observed and predicted dependent values. It assumes a linear relationship and calculates the best-fitting line by adjusting the model's coefficients. The objective is to minimize the overall distance between the observed data points and the line of best fit. This approach enables the model to capture the underlying linear pattern and make predictions based on the learned relationship between the variables."
      ],
      "metadata": {
        "id": "_aH_oVz5llKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_lin = round(mean_squared_error(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse_lin = round(np.sqrt(mse_lin), 4)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae_lin = round(mean_absolute_error(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the R-squared Score (R2)\n",
        "r2_lin = round(r2_score(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the Adjusted R-squared Score (Adjusted R2)\n",
        "adj_r2_lin = round(1 - (1 - r2_lin) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1)), 4)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "evametdf_lin = pd.DataFrame()\n",
        "\n",
        "# Set the 'Metrics' column in the dataframe\n",
        "evametdf_lin['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the 'Linear Regression' column in the dataframe with the corresponding metric values\n",
        "evametdf_lin['Linear Regression'] = [mse_lin, rmse_lin, mae_lin, r2_lin, adj_r2_lin]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lin"
      ],
      "metadata": {
        "id": "zOsbIn6slqAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics for the Linear Regression model are as follows:\n",
        "\n",
        "Mean Squared Error (MSE): The MSE value is 70.4204, indicating the average squared difference between the actual and predicted Close prices. Lower values indicate better model performance, as they represent a smaller overall prediction error.\n",
        "\n",
        "Root Mean Squared Error (RMSE): The RMSE value is 8.3917, which is the square root of the MSE. It provides a measure of the average difference between the actual and predicted Close prices in the original scale. Again, a lower value signifies better predictive accuracy.\n",
        "\n",
        "Mean Absolute Error (MAE): The MAE value is 4.8168, representing the average absolute difference between the actual and predicted Close prices. Similar to MSE and RMSE, a smaller MAE indicates better model performance.\n",
        "\n",
        "R-2 Score: The R-2 score is 0.9937, reflecting the proportion of variance in the dependent variable (Close prices) explained by the independent variables. A score closer to 1 indicates a better fit of the model to the data.\n",
        "\n",
        "Adjusted R-2 Score: The adjusted R-2 score is 0.9931, which considers the number of independent variables and sample size when assessing the model's goodness of fit. This adjustment helps mitigate potential overfitting issues and provides a more reliable measure of model performance.\n",
        "\n",
        "These evaluation metrics collectively demonstrate that the Linear Regression model performs well in predicting the Close prices, with low errors, a high R-2 score, and a relatively stable adjusted R-2 score."
      ],
      "metadata": {
        "id": "nK7PZn06ls7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Lasso Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression, also known as Penalized regression, is a machine learning method commonly used for variable selection. It offers improved prediction accuracy compared to other regression models. By applying Lasso regularization, the model can enhance interpretability while effectively reducing the impact of less relevant variables. This regularization technique plays a crucial role in feature selection and contributes to a more accurate and interpretable model."
      ],
      "metadata": {
        "id": "1OKk86Lpl44-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 0.01)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_lasso = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "mACUreudmAQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coefficients of the Lasso model\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "\n",
        "# Print the intercept of the Lasso model\n",
        "print(\"Intercept:\", lasso.intercept_)"
      ],
      "metadata": {
        "id": "iqzHCCVamCRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Lasso Regression model\n",
        "plt.plot(10**y_pred_lasso, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Lasso Regression\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9S8ReVgfmNPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "VQUc2NDMmRl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression is a regularization technique employed in Linear Regression models. It incorporates a penalty term into the loss function that is based on the sum of the absolute values of the coefficients. This penalty term encourages sparsity in the model by driving some coefficients to exactly zero. As a result, Lasso Regression not only reduces the magnitudes of the coefficients but can also eliminate some features from the model by setting their corresponding coefficients to zero.\n",
        "\n",
        "By reducing the coefficients to zero, Lasso Regression performs feature selection, effectively identifying and prioritizing the most important features for predicting the target variable. This characteristic makes Lasso Regression particularly useful when dealing with high-dimensional datasets where feature reduction is desired.\n",
        "\n",
        "The regularization effect of Lasso Regression helps mitigate overfitting by preventing the model from relying too heavily on any individual feature. It encourages a more parsimonious model representation, improving its generalizability to unseen data. The capability of Lasso Regression to shrink coefficients towards zero and perform feature selection makes it a valuable tool for both improving model interpretability and enhancing prediction accuracy."
      ],
      "metadata": {
        "id": "EA8ls6TgmVjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error\n",
        "mse_lasso = round( mean_squared_error((10**y_test), 10**(y_pred_lasso)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_lasso = round(np.sqrt(mse_lasso), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_lasso = round(mean_absolute_error((10**y_test), 10**(y_pred_lasso)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_lasso = round(r2_score((10**y_test), (10**y_pred_lasso)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_lasso = round(1 - (1 - r2_lasso)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)"
      ],
      "metadata": {
        "id": "zosaML90mhjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for Lasso Regression\n",
        "evametdf_lasso = pd.DataFrame()\n",
        "\n",
        "# Set the 'Metrics' column in the dataframe\n",
        "evametdf_lasso['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the 'Lasso Regression' column in the dataframe with the corresponding metric values\n",
        "evametdf_lasso['Lasso Regression'] = [mse_lasso, rmse_lasso, mae_lasso, r2_lasso, adj_r2_lasso]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lasso"
      ],
      "metadata": {
        "id": "Jo_tlcw3mjoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid for Lasso Regression\n",
        "lasso_param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]}\n",
        "\n",
        "# Perform GridSearchCV with Lasso Regression\n",
        "lasso_gscv = GridSearchCV(lasso, param_grid=lasso_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the Lasso Regression model with GridSearchCV\n",
        "lasso_gscv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the best parameter value\n",
        "print(\"The best value of 'alpha' would be:\", lasso_gscv.best_params_)"
      ],
      "metadata": {
        "id": "c5n7Zt6nmsWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coefficients of the best estimator from GridSearchCV\n",
        "print(\"Coefficients:\", lasso_gscv.best_estimator_.coef_)\n",
        "\n",
        "# Print the intercept of the best estimator from GridSearchCV\n",
        "print(\"Intercept:\", lasso_gscv.best_estimator_.intercept_)"
      ],
      "metadata": {
        "id": "SwdhSIfZmujV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_lasso_gscv = lasso_gscv.predict(X_test)"
      ],
      "metadata": {
        "id": "uKp7e17emw5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Lasso Regression model with GridSearchCV\n",
        "plt.plot(10**y_pred_lasso_gscv, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Lasso Regression with GridSearchCV\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_F5e9bu5m1LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error\n",
        "mse_lasso_gscv = round( mean_squared_error((10**y_test), 10**(y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_lasso_gscv = round(np.sqrt(mse_lasso_gscv), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_lasso_gscv = round(mean_absolute_error((10**y_test), 10**(y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_lasso_gscv = round(r2_score((10**y_test), (10**y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_lasso_gscv = round(1 - (1 - r2_lasso_gscv)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)"
      ],
      "metadata": {
        "id": "HsUHtNNlm4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for Lasso Regression with GridSearchCV\n",
        "evametdf_lasso_gscv = pd.DataFrame()\n",
        "\n",
        "# Add the column \"Metrics\" to the dataframe\n",
        "evametdf_lasso_gscv['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Add the column \"Lasso Regression with GridSearchCV\" to the dataframe with the corresponding evaluation metric values\n",
        "evametdf_lasso_gscv['Lasso Regression with GridSearchCV'] = [mse_lasso_gscv, rmse_lasso_gscv, mae_lasso_gscv, r2_lasso_gscv, adj_r2_lasso_gscv]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lasso_gscv"
      ],
      "metadata": {
        "id": "67hb1YM8m7-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV was used with a smaller set of hyperparameters to find the best combination of hyperparameter values for Lasso Regression. The hyperparameter grid specified a range of alpha values. By narrowing down the set of hyperparameters, the search space was reduced, making the grid search more efficient. GridSearchCV then performed cross-validation to evaluate the performance of each combination of hyperparameters based on the negative mean squared error. The best set of hyperparameters was determined based on the highest cross-validated score, resulting in the optimal regularization strength for Lasso Regression. This approach allowed for an effective and efficient search for the optimal hyperparameters and minimized the mean squared error."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the comparison of evaluation metrics for Lasso Regression and Lasso Regression with GridSearchCV\n",
        "lasso_comp_df = pd.concat([evametdf_lasso, evametdf_lasso_gscv.iloc[:, 1]], axis=1)\n",
        "\n",
        "# Display the dataframe\n",
        "lasso_comp_df"
      ],
      "metadata": {
        "id": "0we-ou-cnV0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression with GridSearchCV is considered the winner due to its lower error metrics and slightly higher R-2 scores. The lower mean squared error, root mean squared error, and mean absolute error indicate improved accuracy and better predictive performance compared to Lasso Regression without GridSearchCV. Additionally, the slightly higher R-2 score suggests that Lasso Regression with GridSearchCV captures a greater amount of variance in the target variable and provides a better fit to the data. Overall, these evaluation metrics demonstrate that Lasso Regression with GridSearchCV outperforms Lasso Regression without GridSearchCV in terms of predictive accuracy and model fit."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 Ridge Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a regularization technique used in multiple regression analysis. While it may seem daunting at first, gaining a solid understanding of multiple regression can provide a foundation for comprehending the science behind Ridge regression in R.\n",
        "\n",
        "In multiple regression, the goal is to build a model that predicts the relationship between a dependent variable and multiple independent variables. This is done by estimating the coefficients of the independent variables that minimize the difference between the predicted and actual values of the dependent variable. The traditional least squares method is commonly used to estimate these coefficients.\n",
        "\n",
        "Ridge regression, on the other hand, introduces a regularization term to the least squares method. This regularization term, known as the Ridge penalty or L2 regularization, adds a constraint to the coefficient estimation process. The purpose of this constraint is to prevent overfitting and improve the model's generalization ability.\n",
        "\n",
        "The Ridge penalty works by adding a weighted sum of squared coefficients to the ordinary least squares cost function. This sum penalizes larger coefficient values, encouraging them to be smaller. Consequently, Ridge regression tends to shrink the coefficient estimates towards zero, while still allowing them to have non-zero values. This shrinkage effect helps mitigate the impact of multicollinearity, a situation where the independent variables are highly correlated with each other.\n",
        "\n",
        "In R, implementing Ridge regression involves specifying a tuning parameter, often denoted as lambda or alpha. This parameter controls the amount of regularization applied to the model. A larger lambda value results in stronger regularization, leading to smaller coefficient estimates. Conversely, a smaller lambda value reduces the regularization effect, allowing the coefficients to approach the values obtained from ordinary least squares regression.\n",
        "\n",
        "By understanding the fundamentals of multiple regression, researchers can grasp the underlying principles of Ridge regression in R. This regularization technique offers a valuable tool for handling multicollinearity and improving the generalization performance of multiple regression models."
      ],
      "metadata": {
        "id": "gvCSS-uGnlgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Ridge regression model from scikit-learn\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create an instance of the Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Fit the Ridge regression model to the training data\n",
        "ridge.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_ridge = ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "UOd74hqpntmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coefficients of the Ridge regression model\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "\n",
        "# Print the intercept of the Ridge regression model\n",
        "print(\"Intercept:\", ridge.intercept_)"
      ],
      "metadata": {
        "id": "Ojb47RwWnwan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Plot the actual Close prices from the test data in blue\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Ridge regression model in red\n",
        "plt.plot(10**y_pred_ridge, color='red')\n",
        "\n",
        "# Set the label for the y-axis as \"Close Price\"\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot as \"Ridge Regression\" with white color\n",
        "plt.title(\"Ridge Regression\", color='white')\n",
        "\n",
        "# Add grid lines to the plot\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sy-Rqi-Vn1P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression is a regularization technique used in Linear Regression models. It introduces a penalty term to the loss function, which is the sum of squared values of the coefficients. This penalty term helps control the magnitude of the coefficients, limiting their impact on the model and reducing the chances of overfitting. By adding this penalty term, Ridge Regression encourages a balance between fitting the training data well and maintaining generalization to unseen data. It is an effective approach to handle multicollinearity and stabilize the model's performance."
      ],
      "metadata": {
        "id": "pX2gvWFDn5ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_ridge = round(mean_squared_error(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse_ridge = round(np.sqrt(mse_ridge), 4)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae_ridge = round(mean_absolute_error(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the R-squared Score (R2 Score)\n",
        "r2_ridge = round(r2_score(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the Adjusted R-squared Score (Adjusted R2 Score)\n",
        "adj_r2_ridge = round(1 - (1 - r2_ridge) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1)), 4)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "evametdf_ridge = pd.DataFrame()\n",
        "\n",
        "# Set the metrics as a column in the dataframe\n",
        "evametdf_ridge['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the corresponding values for Ridge Regression in the dataframe\n",
        "evametdf_ridge['Ridge Regression'] = [mse_ridge, rmse_ridge, mae_ridge, r2_ridge, adj_r2_ridge]\n",
        "\n",
        "evametdf_ridge"
      ],
      "metadata": {
        "id": "qU4YVJhmn9CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid for Ridge regression\n",
        "ridge_param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
        "\n",
        "# Create an instance of the Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Create an instance of GridSearchCV with the Ridge regression model,\n",
        "# the hyperparameter grid, scoring metric, and cross-validation settings\n",
        "ridge_gscv = GridSearchCV(ridge, param_grid=ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the GridSearchCV instance to the training data\n",
        "ridge_gscv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_ridge_gscv = ridge_gscv.predict(X_test)\n"
      ],
      "metadata": {
        "id": "wpUsGJtroGVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the best parameter value\n",
        "print(\"The best value of 'alpha' would be:\", ridge_gscv.best_params_)"
      ],
      "metadata": {
        "id": "faVmssVzoKIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters after GridSearchCV\n",
        "print(\"Coefficients:\", ridge_gscv.best_estimator_.coef_)\n",
        "print(\"Intercept:\", ridge_gscv.best_estimator_.intercept_)"
      ],
      "metadata": {
        "id": "JINAl6mboOL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "plt.gcf().patch.set_facecolor(plot_bgcolor)  # Set the background color of the figure\n",
        "\n",
        "# Plot the actual Close prices from the test set in blue\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Ridge regression model with GridSearchCV in red\n",
        "plt.plot(10**ridge_gscv.predict(X_test), color='red')\n",
        "\n",
        "# Set the y-axis label\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend for the plotted lines\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Add grid lines to the plot\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "# Set the title of the plot with white color\n",
        "plt.title(\"Ridge Regression with GridSearchCV\", color='white')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GEv6HT3GoRru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error\n",
        "mse_ridge_gscv = round( mean_squared_error((10**y_test), 10**(y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_ridge_gscv = round(np.sqrt(mse_ridge_gscv), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_ridge_gscv = round(mean_absolute_error((10**y_test), 10**(y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_ridge_gscv = round(r2_score((10**y_test), (10**y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_ridge_gscv = round(1 - (1 - r2_ridge_gscv)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)"
      ],
      "metadata": {
        "id": "vYxFVapzoVxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty dataframe\n",
        "evametdf_ridge_gscv = pd.DataFrame()\n",
        "\n",
        "# Create a column for the evaluation metrics\n",
        "evametdf_ridge_gscv['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Create a column for the Ridge Regression with GridSearchCV results\n",
        "evametdf_ridge_gscv['Ridge Regression with GridSearchCV'] = [mse_ridge_gscv, rmse_ridge_gscv, mae_ridge_gscv, r2_ridge_gscv, adj_r2_ridge_gscv]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_ridge_gscv"
      ],
      "metadata": {
        "id": "Fb-clBy5oYjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason GridSearchCV was used in this code is that we are working with a smaller set of hyperparameters for the Ridge regression model. GridSearchCV allows us to exhaustively search through the specified hyperparameter grid and find the best combination of hyperparameters that yields the optimal model performance.\n",
        "\n",
        "In this case, the hyperparameter being tuned is the alpha parameter, which represents the regularization strength in Ridge regression. The ridge_param_grid contains a predefined list of potential alpha values to explore. By using GridSearchCV, the code iterates through each alpha value in the grid, fits the Ridge regression model with that particular alpha, and evaluates the model's performance using cross-validation.\n",
        "\n",
        "GridSearchCV is an effective approach when dealing with a smaller hyperparameter space because it systematically evaluates every possible combination within that space. However, as the hyperparameter space grows larger, GridSearchCV may become computationally expensive and time-consuming.\n",
        "\n",
        "It's important to note that the choice of hyperparameter search method depends on the specific problem, the size of the hyperparameter space, and the available computational resources. GridSearchCV is suitable for smaller hyperparameter spaces, while other techniques like RandomizedSearchCV or Bayesian optimization may be more efficient for larger hyperparameter spaces.\n",
        "\n",
        "Overall, GridSearchCV provides a systematic way to search through a smaller set of hyperparameters and identify the optimal combination for the Ridge regression model, leading to improved model performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating two DataFrames side by side using pd.concat()\n",
        "# Here, we are combining 'evametdf_ridge' and the second column ('iloc[:, 1]') of 'evametdf_ridge_gscv' DataFrame.\n",
        "ridge_comp_df = pd.concat([evametdf_ridge, evametdf_ridge_gscv.iloc[:, 1]], axis=1)\n",
        "\n",
        "# Displaying the resulting DataFrame after concatenation\n",
        "ridge_comp_df"
      ],
      "metadata": {
        "id": "5ObpoBdYoiGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of error metrics, the Ridge Regression model with GridSearchCV outperformed other models. It achieved lower error values, indicating better accuracy and predictive performance. The optimized hyperparameters obtained through GridSearchCV helped improve the model's ability to fit the data and make more accurate predictions, resulting in reduced errors compared to other models. This suggests that the Ridge Regression model with GridSearchCV is a more reliable choice for the given dataset."
      ],
      "metadata": {
        "id": "pUnPZP3Ooluu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A careful examination of the data reveals a pronounced decline in the stock prices of Yes Bank following the exposure of the Rana Kapoor fraud in 2018.\n",
        "\n",
        "The dataset exhibited exceptional cleanliness, devoid of any missing values or duplicated rows, minimizing the need for extensive data wrangling.\n",
        "\n",
        "Although outliers were present in the features, effective outlier mitigation was achieved through the implementation of a log transformation across all features.\n",
        "\n",
        "The log transformation successfully addressed positive skewness observed in all features, ensuring adherence to the assumptions of the linear regression models.\n",
        "\n",
        "Strong positive correlations were observed between the independent variables (Open, High, Low) and the dependent variable (Close), implying a high predictive potential of the dependent variable based on the independent variables.\n",
        "\n",
        "The presence of positive correlations among the independent variables suggested the presence of multicollinearity; however, given the limited dataset size, feature removal was deemed unnecessary.\n",
        "\n",
        "Among the various implemented regression models, the Ridge Regression model, combined with GridSearchCV for hyperparameter optimization, emerged as the preferred choice. It achieved a commendable performance, boasting an RMSE of 8.3824 and an R-2 score of 0.9938.\n",
        "\n",
        "Notably, the 'High' and 'Low' features demonstrated positive weights, indicating a favorable impact on the predictions. Conversely, the 'Open' feature displayed a negative weight, signifying a detrimental influence on the predictions.\n",
        "\n",
        "Satisfactorily meeting the assumptions of homoscedasticity, absence of autocorrelation, and a mean of zero, the residuals bolstered the reliability of the regression model."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}